import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

def test_bank_data_loading():
    """æµ‹è¯•Bank Marketingæ•°æ®é›†åŠ è½½"""
    
    print("="*60)
    print("æµ‹è¯•Bank Marketingæ•°æ®é›†åŠ è½½")
    print("="*60)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = './data/bank/bank-additional/bank-additional.csv'
    full_data_path = './data/bank/bank-additional/bank-additional-full.csv'
    
    print(f"æ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    print(f"å°æ•°æ®é›†è·¯å¾„: {data_path}")
    print(f"å®Œæ•´æ•°æ®é›†è·¯å¾„: {full_data_path}")
    print(f"å°æ•°æ®é›†å­˜åœ¨: {os.path.exists(data_path)}")
    print(f"å®Œæ•´æ•°æ®é›†å­˜åœ¨: {os.path.exists(full_data_path)}")
    
    if not os.path.exists(data_path):
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼")
        return False
    
    # åŠ è½½æ•°æ®
    print(f"\næ­£åœ¨åŠ è½½æ•°æ®...")
    try:
        # æ³¨æ„ï¼šBank Marketingæ•°æ®é›†ä½¿ç”¨åˆ†å·ä½œä¸ºåˆ†éš”ç¬¦
        df = pd.read_csv(data_path, sep=';')
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
        
        # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
        print(f"\næ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
        print(f"  å½¢çŠ¶: {df.shape}")
        print(f"  åˆ—æ•°: {len(df.columns)}")
        print(f"  è¡Œæ•°: {len(df)}")
        
        print(f"\nåˆ—å:")
        for i, col in enumerate(df.columns, 1):
            print(f"  {i:2d}. {col}")
        
        # æ˜¾ç¤ºç›®æ ‡å˜é‡åˆ†å¸ƒ
        target_col = 'y'
        if target_col in df.columns:
            print(f"\nç›®æ ‡å˜é‡ '{target_col}' åˆ†å¸ƒ:")
            value_counts = df[target_col].value_counts()
            for value, count in value_counts.items():
                percentage = count / len(df) * 100
                print(f"  {value}: {count} ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
        print(f"\nå‰5è¡Œæ•°æ®é¢„è§ˆ:")
        print(df.head())
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        print(f"\næ•°æ®ç±»å‹:")
        categorical_cols = []
        numerical_cols = []
        
        for col in df.columns:
            if col == target_col:
                continue
            dtype = df[col].dtype
            if dtype == 'object' or df[col].dtype.name == 'category':
                categorical_cols.append(col)
                unique_count = df[col].nunique()
                print(f"  {col}: åˆ†ç±» ({unique_count} ä¸ªå”¯ä¸€å€¼)")
            else:
                numerical_cols.append(col)
                print(f"  {col}: æ•°å€¼")
        
        print(f"\nç‰¹å¾ç±»å‹ç»Ÿè®¡:")
        print(f"  åˆ†ç±»ç‰¹å¾: {len(categorical_cols)} ä¸ª")
        print(f"  æ•°å€¼ç‰¹å¾: {len(numerical_cols)} ä¸ª")
        print(f"  æ€»ç‰¹å¾æ•°: {len(categorical_cols) + len(numerical_cols)} ä¸ª")
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        print(f"\nç¼ºå¤±å€¼æ£€æŸ¥:")
        missing_data = df.isnull().sum()
        if missing_data.sum() == 0:
            print("  âœ… æ²¡æœ‰ç¼ºå¤±å€¼")
        else:
            print("  ç¼ºå¤±å€¼ç»Ÿè®¡:")
            for col, missing_count in missing_data.items():
                if missing_count > 0:
                    percentage = missing_count / len(df) * 100
                    print(f"    {col}: {missing_count} ({percentage:.1f}%)")
        
        # æµ‹è¯•é¢„å¤„ç†
        print(f"\næµ‹è¯•æ•°æ®é¢„å¤„ç†...")
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        features = df.drop(columns=[target_col])
        labels = df[target_col]
        
        # ç¼–ç åˆ†ç±»ç‰¹å¾
        print("  ç¼–ç åˆ†ç±»ç‰¹å¾...")
        processed_features = features.copy()
        
        for col in categorical_cols:
            le = LabelEncoder()
            processed_features[col] = le.fit_transform(processed_features[col].astype(str))
            print(f"    {col}: {len(le.classes_)} ä¸ªç±»åˆ«")
        
        # ç¼–ç ç›®æ ‡æ ‡ç­¾
        print("  ç¼–ç ç›®æ ‡æ ‡ç­¾...")
        label_encoder = LabelEncoder()
        processed_labels = label_encoder.fit_transform(labels)
        print(f"    ç›®æ ‡ç±»åˆ«: {label_encoder.classes_}")
        
        # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
        print("  æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾...")
        scaler = StandardScaler()
        processed_features[numerical_cols] = scaler.fit_transform(processed_features[numerical_cols])
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        X = processed_features.values.astype(np.float32)
        y = processed_labels.astype(np.int64)
        
        print(f"\né¢„å¤„ç†ç»“æœ:")
        print(f"  ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
        print(f"  æ ‡ç­¾å‘é‡å½¢çŠ¶: {y.shape}")
        print(f"  æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
        
        # æµ‹è¯•æ•°æ®åˆ’åˆ†
        print(f"\næµ‹è¯•æ•°æ®åˆ’åˆ†...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"  è®­ç»ƒé›†: {X_train.shape}")
        print(f"  æµ‹è¯•é›†: {X_test.shape}")
        print(f"  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_train)}")
        print(f"  æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y_test)}")
        
        print(f"\nâœ… æ•°æ®é¢„å¤„ç†æµ‹è¯•æˆåŠŸï¼")
        
        # ä¸ºVFLå‡†å¤‡æ•°æ®
        print(f"\nä¸ºVFLå‡†å¤‡æ•°æ®åˆ†å‰²...")
        num_parties = 3
        feature_dim = X.shape[1]
        features_per_party = feature_dim // num_parties
        
        print(f"  æ€»ç‰¹å¾æ•°: {feature_dim}")
        print(f"  å‚ä¸æ–¹æ•°é‡: {num_parties}")
        print(f"  æ¯æ–¹ç‰¹å¾æ•°: {features_per_party}")
        
        for i in range(num_parties):
            start_idx = i * features_per_party
            if i == num_parties - 1:
                end_idx = feature_dim  # æœ€åä¸€æ–¹è·å¾—å‰©ä½™ç‰¹å¾
            else:
                end_idx = (i + 1) * features_per_party
            
            party_features = X_train[:, start_idx:end_idx]
            print(f"  å‚ä¸æ–¹ {i}: ç‰¹å¾èŒƒå›´ [{start_idx}:{end_idx}], ç»´åº¦ {party_features.shape[1]}")
        
        print(f"\nâœ… VFLæ•°æ®åˆ†å‰²æµ‹è¯•æˆåŠŸï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return False

def create_processed_data():
    """åˆ›å»ºé¢„å¤„ç†åçš„æ•°æ®æ–‡ä»¶ä¾›è®­ç»ƒä½¿ç”¨"""
    
    print(f"\n" + "="*60)
    print("åˆ›å»ºé¢„å¤„ç†æ•°æ®æ–‡ä»¶")
    print("="*60)
    
    # æ•°æ®è·¯å¾„
    data_path = './data/bank/bank-additional/bank-additional-full.csv'
    if not os.path.exists(data_path):
        data_path = './data/bank/bank-additional/bank-additional.csv'
    
    if not os.path.exists(data_path):
        print("âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
        return False
    
    # åŠ è½½æ•°æ®
    print(f"ä» {data_path} åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path, sep=';')
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # é¢„å¤„ç†
    target_col = 'y'
    features = df.drop(columns=[target_col])
    labels = df[target_col]
    
    # åˆ†ç±»ç‰¹å¾ç¼–ç 
    categorical_columns = features.select_dtypes(include=['object']).columns
    numerical_columns = features.select_dtypes(include=['int64', 'float64']).columns
    
    print(f"åˆ†ç±»ç‰¹å¾: {len(categorical_columns)} ä¸ª")
    print(f"æ•°å€¼ç‰¹å¾: {len(numerical_columns)} ä¸ª")
    
    # ç¼–ç åˆ†ç±»ç‰¹å¾
    for col in categorical_columns:
        le = LabelEncoder()
        features[col] = le.fit_transform(features[col].astype(str))
    
    # ç¼–ç æ ‡ç­¾
    label_encoder = LabelEncoder()
    labels_encoded = label_encoder.fit_transform(labels)
    
    # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
    scaler = StandardScaler()
    features[numerical_columns] = scaler.fit_transform(features[numerical_columns])
    
    # è½¬æ¢æ•°æ®ç±»å‹
    X = features.values.astype(np.float32)
    y = labels_encoded.astype(np.int64)
    
    # åˆ’åˆ†æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = './data/bank'
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ•°æ®
    print(f"ä¿å­˜é¢„å¤„ç†æ•°æ®åˆ° {output_dir}...")
    np.save(os.path.join(output_dir, 'X_train.npy'), X_train)
    np.save(os.path.join(output_dir, 'X_test.npy'), X_test)
    np.save(os.path.join(output_dir, 'y_train.npy'), y_train)
    np.save(os.path.join(output_dir, 'y_test.npy'), y_test)
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'feature_names': list(features.columns),
        'categorical_columns': list(categorical_columns),
        'numerical_columns': list(numerical_columns),
        'target_classes': list(label_encoder.classes_),
        'feature_dim': X.shape[1],
        'num_classes': len(label_encoder.classes_),
        'train_samples': len(X_train),
        'test_samples': len(X_test)
    }
    
    import pickle
    with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼")
    print(f"è®­ç»ƒé›†: {X_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    print(f"ç‰¹å¾ç»´åº¦: {metadata['feature_dim']}")
    print(f"ç±»åˆ«æ•°: {metadata['num_classes']}")
    
    return True

if __name__ == '__main__':
    # æµ‹è¯•æ•°æ®åŠ è½½
    success = test_bank_data_loading()
    
    if success:
        # åˆ›å»ºé¢„å¤„ç†æ•°æ®
        create_processed_data()
        
        print(f"\n" + "="*60)
        print("ğŸ‰ Bank Marketingæ•°æ®é›†éªŒè¯å®Œæˆï¼")
        print("="*60)
        print("æ•°æ®é›†å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒVILLAINæ”»å‡»")
        print()
        print("è¿è¡Œè®­ç»ƒå‘½ä»¤:")
        print("python train_bank_villain_with_inference.py \\")
        print("  --dataset BANK \\")
        print("  --data-dir ./data/bank \\")
        print("  --batch-size 64 \\")
        print("  --epochs 50 \\")
        print("  --lr 0.002 \\")
        print("  --trigger-size 0.08 \\")
        print("  --trigger-magnitude 0.4 \\")
        print("  --poison-budget 0.06 \\")
        print("  --inference-weight 0.15 \\")
        print("  --confidence-threshold 0.35 \\")
        print("  --Ebkd 5 \\")
        print("  --warmup-epochs 4 \\")
        print("  --early-stopping \\")
        print("  --gpu 0")
        
    else:
        print("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶") 